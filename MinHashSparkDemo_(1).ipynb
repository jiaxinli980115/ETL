{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "MinHashSparkDemo (1).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jiaxinli980115/ETL/blob/main/MinHashSparkDemo_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2DecDzwMs7y"
      },
      "source": [
        "# Example use of MinHash for deduplication"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TisxaNGpMs72"
      },
      "source": [
        "### Load apple dataset from json"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kpOeO3rxOklQ",
        "outputId": "74eec60b-d688-4fd3-eabd-05aec1a2560d"
      },
      "source": [
        "!pip install pyspark"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/b0/9d6860891ab14a39d4bddf80ba26ce51c2f9dc4805e5c6978ac0472c120a/pyspark-3.1.1.tar.gz (212.3MB)\n",
            "\u001b[K     |████████████████████████████████| 212.3MB 66kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 20.5MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.1.1-py2.py3-none-any.whl size=212767604 sha256=10b8a8f12d6276cd9e25dbe91def4ecbf5db9b6b740cb389faf52f71bba0ee22\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/90/c0/01de724414ef122bd05f056541fb6a0ecf47c7ca655f8b3c0f\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9 pyspark-3.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4jZwqs7Ms73"
      },
      "source": [
        "import json\n",
        "\n",
        "boeing_data = []\n",
        "with open('webhose_boeing.json', 'r') as f:\n",
        "    for line in f.readlines():\n",
        "        boeing_data.append(json.loads(line))\n",
        "\n",
        "boeing_titles = [a['title'] for a in boeing_data]"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zquPpmsHMs74",
        "outputId": "fea17d8d-5693-47d1-db2b-01e6ec7be824"
      },
      "source": [
        "%env PYSPARK_PYTHON=python3"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "env: PYSPARK_PYTHON=python3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_udwEZvHMs74"
      },
      "source": [
        "### Set comes with duplicates:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PXmYBf4cMs74",
        "outputId": "9a91c4d3-223f-42fe-85d1-ad8c536be918"
      },
      "source": [
        "len(boeing_titles), len(set(boeing_titles))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7484, 4866)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFLsW3MIMs75"
      },
      "source": [
        "### This is a toy example, and exact duplicates can be found with python's set object. But what if it was bigger? \n",
        "\n",
        "### We can use spark's MinHash over character n-grams\n",
        "#### We create a dataframe containing the title as a string in one column and a list of characters in another"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKF0RjN6Ms75",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eef4da48-3db1-473a-ca4f-4d619f0dbc4d"
      },
      "source": [
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "sc = SparkContext()\n",
        "spark = SparkSession(sc)\n",
        "df = spark.createDataFrame([\n",
        "    (k, t, list(t)) for k, t in enumerate(boeing_titles) if len(list(t)) >=3],\n",
        "    ['id', 'title', 'title_characters'])\n",
        "df.select('title').show()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+\n",
            "|               title|\n",
            "+--------------------+\n",
            "|FAA chief called ...|\n",
            "|FAA chief called ...|\n",
            "|Cathay Pacific fi...|\n",
            "|B-2 to test GBU-5...|\n",
            "|Boeing rejected 7...|\n",
            "|Boeing files arbi...|\n",
            "|Meet The Oldest B...|\n",
            "|Five Reasons The ...|\n",
            "|FAA chief called ...|\n",
            "|Why Is Flying In ...|\n",
            "|FAA chief called ...|\n",
            "|FAA chief called ...|\n",
            "|FAA chief called ...|\n",
            "|FAA chief called ...|\n",
            "|FAA chief called ...|\n",
            "|FAA chief called ...|\n",
            "|FAA chief called ...|\n",
            "|FAA chief called ...|\n",
            "|FAA chief called ...|\n",
            "|Today in History,...|\n",
            "+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UpNbnGeZMs75"
      },
      "source": [
        "#### Now we use spark to select character n-grams (shingles)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9kKnWBmGMs76",
        "outputId": "d592a72c-6e30-4da8-8ede-35c0e9605000"
      },
      "source": [
        "from pyspark.ml.feature import NGram\n",
        "\n",
        "ngram = NGram(n=3, inputCol='title_characters', outputCol='ngrams')\n",
        "ngram_df = ngram.transform(df)\n",
        "ngram_df.select('ngrams').show()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+\n",
            "|              ngrams|\n",
            "+--------------------+\n",
            "|[F A A, A A  , A ...|\n",
            "|[F A A, A A  , A ...|\n",
            "|[C a t, a t h, t ...|\n",
            "|[B - 2, - 2  , 2 ...|\n",
            "|[B o e, o e i, e ...|\n",
            "|[B o e, o e i, e ...|\n",
            "|[M e e, e e t, e ...|\n",
            "|[F i v, i v e, v ...|\n",
            "|[F A A, A A  , A ...|\n",
            "|[W h y, h y  , y ...|\n",
            "|[F A A, A A  , A ...|\n",
            "|[F A A, A A  , A ...|\n",
            "|[F A A, A A  , A ...|\n",
            "|[F A A, A A  , A ...|\n",
            "|[F A A, A A  , A ...|\n",
            "|[F A A, A A  , A ...|\n",
            "|[F A A, A A  , A ...|\n",
            "|[F A A, A A  , A ...|\n",
            "|[F A A, A A  , A ...|\n",
            "|[T o d, o d a, d ...|\n",
            "+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0N3R02lMs77"
      },
      "source": [
        "#### And transform those in-grams into binary vectors we can use in MinHash"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7C9YPOkEMs77",
        "outputId": "f6d3fb5b-5bec-4186-a4b7-6ad00f2d2148"
      },
      "source": [
        "from pyspark.ml.feature import CountVectorizer \n",
        "\n",
        "count_vectorizer = CountVectorizer(inputCol='ngrams', outputCol='vector', binary=True)\n",
        "model = count_vectorizer.fit(ngram_df)\n",
        "cv_df = model.transform(ngram_df)\n",
        "\n",
        "# the vectors are displayed in 'sparse' format, \n",
        "# i.e. the numbers shown are the indices i of vector x where x[i]=1, \n",
        "# and x[k]=0 for all other k\n",
        "cv_df.select('vector').show()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+\n",
            "|              vector|\n",
            "+--------------------+\n",
            "|(14930,[0,1,3,4,5...|\n",
            "|(14930,[0,1,3,4,5...|\n",
            "|(14930,[1,2,24,29...|\n",
            "|(14930,[8,9,15,21...|\n",
            "|(14930,[0,1,2,3,4...|\n",
            "|(14930,[0,1,2,3,4...|\n",
            "|(14930,[0,1,3,4,5...|\n",
            "|(14930,[12,14,15,...|\n",
            "|(14930,[0,1,3,4,5...|\n",
            "|(14930,[0,1,38,54...|\n",
            "|(14930,[0,1,3,4,5...|\n",
            "|(14930,[0,1,3,4,5...|\n",
            "|(14930,[0,1,3,4,5...|\n",
            "|(14930,[0,1,3,4,5...|\n",
            "|(14930,[0,1,3,4,5...|\n",
            "|(14930,[0,1,3,4,5...|\n",
            "|(14930,[0,1,3,4,5...|\n",
            "|(14930,[0,1,3,4,5...|\n",
            "|(14930,[0,1,3,4,5...|\n",
            "|(14930,[6,10,73,2...|\n",
            "+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8rqaBMCMs78"
      },
      "source": [
        "#### Now we can apply MinHash"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JetKjMuuMs78",
        "outputId": "f294c468-890e-4398-baff-04b280e05ec2"
      },
      "source": [
        "from pyspark.ml.feature import MinHashLSH\n",
        "\n",
        "min_hash = MinHashLSH(inputCol='vector', outputCol='minHash', seed=0, numHashTables=10)\n",
        "model = min_hash.fit(cv_df)\n",
        "hash_df = model.transform(cv_df)\n",
        "\n",
        "# We now have the min hash values for the dataset of article titles\n",
        "hash_df.select('minHash').show(1)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+\n",
            "|             minHash|\n",
            "+--------------------+\n",
            "|[[6333835.0], [23...|\n",
            "+--------------------+\n",
            "only showing top 1 row\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHzlIZvaMs79"
      },
      "source": [
        "#### We can now use these values to search for duplicates for a given title: everything with Jaccard similarity above a given threshold is probably similar"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tkoImireMs7-",
        "outputId": "fa032345-0888-46f8-de55-75a53df37056"
      },
      "source": [
        "joined_rows = model.approxSimilarityJoin(cv_df, cv_df, threshold=0.05, distCol='jaccard_distance')\n",
        "# the returned dataframe will be pairs of rows where each pair has an estimated distance of at most the threshold\n",
        "joined_rows.show()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+--------------------+----------------+\n",
            "|            datasetA|            datasetB|jaccard_distance|\n",
            "+--------------------+--------------------+----------------+\n",
            "|{5974, REPORT: Ma...|{5974, REPORT: Ma...|             0.0|\n",
            "|{6859, Virgin Atl...|{6866, Virgin Atl...|             0.0|\n",
            "|{6866, Virgin Atl...|{6886, Virgin Atl...|             0.0|\n",
            "|{6868, Virgin Atl...|{6875, Virgin Atl...|             0.0|\n",
            "|{6872, Virgin Atl...|{6895, Virgin Atl...|             0.0|\n",
            "|{6876, Virgin Atl...|{6864, Virgin Atl...|             0.0|\n",
            "|{6883, Virgin Atl...|{6887, Virgin Atl...|             0.0|\n",
            "|{6889, Virgin Atl...|{6882, Virgin Atl...|             0.0|\n",
            "|{6895, Virgin Atl...|{6880, Virgin Atl...|             0.0|\n",
            "|{1043, Boeing Cut...|{1295, Boeing Cut...|             0.0|\n",
            "|{1266, Boeing Cut...|{1284, Boeing Cut...|             0.0|\n",
            "|{1278, Boeing Cut...|{1294, Boeing Cut...|             0.0|\n",
            "|{1279, Boeing Cut...|{1273, Boeing Cut...|             0.0|\n",
            "|{1280, Boeing Cut...|{1288, Boeing Cut...|             0.0|\n",
            "|{1283, Boeing Cut...|{1292, Boeing Cut...|             0.0|\n",
            "|{1285, Boeing Cut...|{1281, Boeing Cut...|             0.0|\n",
            "|{1292, Boeing Cut...|{1286, Boeing Cut...|             0.0|\n",
            "|{1295, Boeing Cut...|{1282, Boeing Cut...|             0.0|\n",
            "|{1305, Boeing Cut...|{1299, Boeing Cut...|             0.0|\n",
            "|{6233, Royal Aust...|{6233, Royal Aust...|             0.0|\n",
            "+--------------------+--------------------+----------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cP220ILbMs7_"
      },
      "source": [
        "#### We can now see all duplicates of a title"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JBiXj9FgMs8A",
        "outputId": "5ad4fa9d-c4bd-4fb1-9eeb-02f0daea341a"
      },
      "source": [
        "from pyspark.sql.functions import col\n",
        "joined_rows.filter(joined_rows.datasetA.id == 2467).select(col('datasetA.title'), col('datasetB.title')).show()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+--------------------+\n",
            "|               title|               title|\n",
            "+--------------------+--------------------+\n",
            "|Boeing prepares t...|Boeing prepares t...|\n",
            "+--------------------+--------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ajlB3VEMs8A"
      },
      "source": [
        "#### And we can deduplicate the whole dataset if we want to"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYsg4-e2Ms8B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "781145d7-80cd-4c9c-f465-346df5d180a5"
      },
      "source": [
        "# this will return the first id for each title in the dataset\n",
        "deduplicated_df = joined_rows.groupby(col('datasetA.title')).min('datasetA.id')\n",
        "deduplicated_df.show(5)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+------------------------+\n",
            "|      datasetA.title|min(datasetA.id AS `id`)|\n",
            "+--------------------+------------------------+\n",
            "|Warren Buffett is...|                    5137|\n",
            "|FAA outlines refo...|                    3415|\n",
            "|Lawmakers Push fo...|                    3235|\n",
            "|NASA Chief “All I...|                    1711|\n",
            "|Boeing expects to...|                    6374|\n",
            "+--------------------+------------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BUAyYEsEMs8B",
        "outputId": "6adca60c-eaa6-4398-c1e4-5b5054a61b7c"
      },
      "source": [
        "deduplicated_titles = deduplicated_df.toPandas()['datasetA.title']\n",
        "\n",
        "# we can check that the resulting list is the right length\n",
        "# or almost the same length, it's not exact\n",
        "len(deduplicated_titles), len(boeing_titles), len(set(boeing_titles))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4866, 7484, 4866)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MoiI_8CKHxn"
      },
      "source": [
        "deduplicated_df = deduplicated_df.toPandas()\n",
        "deduplicated_df.rename(columns={'datasetA.title':'title','min(datasetA.id AS `id`)':'id'}, inplace = True)\n",
        "unique_id = deduplicated_df['id'].values\n",
        "boeing = list(boeing_data[i] for i in unique_id)\n",
        "with open('deduplicated_titles','w') as f:\n",
        "            json.dump(boeing,f)"
      ],
      "execution_count": 13,
      "outputs": []
    }
  ]
}